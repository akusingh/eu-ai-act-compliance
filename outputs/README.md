# Outputs Directory

This directory contains evaluation results, metrics, and traces from agent execution.

## Output Files

### Evaluation Results
- **`evaluation.json`** - Detailed evaluation results for each test scenario
  - Query text and AI system description
  - Expected vs actual classifications
  - Compliance assessment details
  - Success/failure indicators

### Performance Metrics
- **`metrics.json`** - Aggregated performance metrics
  - Accuracy scores
  - Execution times
  - Success rates
  - Agent performance breakdown

### Execution Traces
- **`traces.json`** - Detailed execution traces
  - Agent interactions
  - Tool calls and results
  - Reasoning steps
  - Timing information

## File Structure

### evaluation.json
```json
{
  "scenario_name": "facial_recognition_law_enforcement",
  "query": "facial recognition for law enforcement",
  "expected": {
    "risk_level": "prohibited",
    "articles": ["Article 5(1)(d)"]
  },
  "actual": {
    "risk_level": "prohibited",
    "confidence": 0.95,
    "reasoning": "..."
  },
  "success": true,
  "execution_time": 12.5
}
```

### metrics.json
```json
{
  "total_scenarios": 10,
  "successful": 9,
  "accuracy": 0.90,
  "avg_execution_time": 15.2,
  "breakdown": {
    "prohibited": {"count": 3, "accuracy": 1.0},
    "high_risk": {"count": 5, "accuracy": 0.85},
    "limited_risk": {"count": 2, "accuracy": 0.90}
  }
}
```

## Usage

These files are generated by:

```bash
# Run evaluation suite
python evaluate.py

# Generates:
# - outputs/evaluation.json
# - outputs/metrics.json  
# - outputs/traces.json
```

## Analyzing Results

### View Evaluation Summary
```bash
# Pretty print metrics
cat outputs/metrics.json | jq '.'

# Check accuracy
cat outputs/metrics.json | jq '.accuracy'

# View failed scenarios
cat outputs/evaluation.json | jq '.[] | select(.success == false)'
```

### Find Performance Issues
```bash
# Scenarios with slow execution
cat outputs/evaluation.json | jq '.[] | select(.execution_time > 20)'

# Scenarios with low confidence
cat outputs/evaluation.json | jq '.[] | select(.actual.confidence < 0.7)'
```

## Integration with Kaggle

These output files can be used to demonstrate:
- **Agent Accuracy** - Compliance assessment correctness
- **Performance** - Execution speed and efficiency
- **Reliability** - Consistency across test scenarios
- **Explainability** - Detailed reasoning traces

## Cleanup

Outputs are regenerated on each evaluation run. To archive results:

```bash
# Create backup with timestamp
timestamp=$(date +%Y%m%d_%H%M%S)
mkdir -p outputs/archive
cp outputs/*.json outputs/archive/results_$timestamp/
```

## Note

This directory is included in `.gitignore` to prevent committing large result files. Only commit summary results or particularly important evaluation runs.
